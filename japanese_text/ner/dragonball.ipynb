{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sed -i -e '/^[0-9]/d' Dragon*\n",
    "# sed -i '/^$/d' Dragon*\n",
    "# sed -i 1,3d Dragon*\n",
    "#from __future__ import unicode_literals, print_function\n",
    "\n",
    "\n",
    "import jparse\n",
    "from __future__ import print_function, unicode_literals\n",
    "\n",
    "\n",
    "with open('dragonball/Dragon_Ball_Super_001.srt', 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19618"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "＜史上最強の敵魔人ブウの出現により地球は　絶体絶命の危機に見舞われた＞\r\n",
      "\r\n",
      "＜魔人ブウに立ち向かう孫　悟空と　ベジータ＞\r\n",
      "\r\n",
      "＜サタンの呼び掛けで地球の　みんなと全宇宙の元気を集めたスーパー元気玉でついに　悟空は魔人ブウを倒したのだ＞\r\n",
      "\r\n",
      "＜それから　半年＞\r\n",
      "\r\n",
      "＜ドラゴンボールの力により人々から　ブウの記憶は消えしばらくの月日が流れた＞\r\n",
      "\r\n",
      "（悟空）えっ？あっ。\r\n",
      "\r\n",
      "（悟空）はぁ…。おぉ。お前 ベジータ みてぇに頑固なやつだな。 ヘッ。 よっ。\r\n",
      "\r\n",
      "（悟天）お父さーん。\r\n",
      "（悟空）うん？\r\n",
      "\r\n",
      "（悟天）お弁当だよー。\r\n",
      "\r\n",
      "（悟空）おう　悟天。\r\n",
      "（悟空）何だ　学校は　もう終わったんか？\r\n",
      "\r\n",
      "（悟天）今日は　日曜日だよ。\r\n",
      "\r\n",
      "（悟空）休みか。　あっ。　ヒヒヒ。\r\n",
      "\r\n",
      "（悟空）じゃあ　ちょっと　代わりにこいつ　運転してくれ。\r\n",
      "\r\n",
      "（悟天）いいけど。\r\n",
      "\r\n",
      "（悟空）おぉ！　弁当　弁当。おぉー！ いただきます。 いやー食ったー！ ごちそうさまでした 。よし。 ふっ\n"
     ]
    }
   ],
   "source": [
    "print(data[:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = data.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [b for b in sentences if b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "＜史上最強の敵魔人ブウの出現により地球は　絶体絶命の危機に見舞われた＞\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/characters.txt','r') as f:\n",
    "    characters = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = characters.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [b for b in characters if b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "characters = ['悟天','悟空','サタン','魔人ブウ','ビルス','ウイス','ブウ','ビーデル','悟飯',\n",
    "              'ピッコロ','トランクス','ブルマ','チチ','亀仙人','キビト神','老界王神','ベジータ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "character_dictionary = {}\n",
    "for index, term in enumerate(characters):\n",
    "    character_dictionary[index] = term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "悟天\n"
     ]
    }
   ],
   "source": [
    "print(character_dictionary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_corpus(sentences, terms):\n",
    "    corpus = []\n",
    "    for term in terms:\n",
    "        corpus.append([b for b in sentences if term in b])\n",
    "    return corpus\n",
    "\n",
    "def annotate_training(corpus, dictionary, name):\n",
    "    \"\"\"\n",
    "    Args: clean lower cased sentences, skills, entity category\n",
    "    Returns: list of annotated sentences\n",
    "    \"\"\"\n",
    "    train = []\n",
    "    for b, _list in enumerate(corpus):\n",
    "        for sentence in _list:\n",
    "            train.extend([(int(sentence.index(dictionary[b])),\n",
    "                           int(sentence.index(dictionary[b]) + len(dictionary[b])), \n",
    "                           name)])\n",
    "    corpus = [item for sublist in corpus for item in sublist]\n",
    "    return zip(corpus, train)\n",
    "    \n",
    "\n",
    "def removeNonAscii(data):\n",
    "    \"\"\" use to remove stubborn non ascii characters from text; relevant in Python2\n",
    "    example on dataframe: [removeNonAscii(b) for b in df['column']]\n",
    "    example on json: [removeNonAscii(b['item']) for b in json]\n",
    "    \"\"\"\n",
    "    return \"\".join(i for i in data if ord(i)<128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xef\\xbc\\x9c\\xe5\\x8f\\xb2\\xe4\\xb8\\x8a\\xe6\\x9c\\x80\\xe5\\xbc\\xb7\\xe3\\x81\\xae\\xe6\\x95\\xb5\\xe9\\xad\\x94\\xe4\\xba\\xba\\xe3\\x83\\x96\\xe3\\x82\\xa6\\xe3\\x81\\xae\\xe5\\x87\\xba\\xe7\\x8f\\xbe\\xe3\\x81\\xab\\xe3\\x82\\x88\\xe3\\x82\\x8a\\xe5\\x9c\\xb0\\xe7\\x90\\x83\\xe3\\x81\\xaf\\xe3\\x80\\x80\\xe7\\xb5\\xb6\\xe4\\xbd\\x93\\xe7\\xb5\\xb6\\xe5\\x91\\xbd\\xe3\\x81\\xae\\xe5\\x8d\\xb1\\xe6\\xa9\\x9f\\xe3\\x81\\xab\\xe8\\xa6\\x8b\\xe8\\x88\\x9e\\xe3\\x82\\x8f\\xe3\\x82\\x8c\\xe3\\x81\\x9f\\xef\\xbc\\x9e'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'\\uff1c\\u53f2\\u4e0a\\u6700\\u5f37\\u306e\\u6575\\u9b54\\u4eba\\u30d6\\u30a6\\u306e\\u51fa\\u73fe\\u306b\\u3088\\u308a\\u5730\\u7403\\u306f\\u3000\\u7d76\\u4f53\\u7d76\\u547d\\u306e\\u5371\\u6a5f\\u306b\\u898b\\u821e\\u308f\\u308c\\u305f\\uff1e'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u'＜史上最強の敵魔人ブウの出現により地球は　絶体絶命の危機に見舞われた＞'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = create_corpus(sentences, characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = annotate_training(corpus, character_dictionary, 'character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（悟天）お父さーん。\n"
     ]
    }
   ],
   "source": [
    "print(train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/db_train.txt', 'w') as f:\n",
    "    for tup in train:\n",
    "        f.write(str(tup)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import random\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import ast\n",
    "\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from spacy.gold import GoldParse, biluo_tags_from_offsets\n",
    "\n",
    "\n",
    "from spacy.tagger import Tagger\n",
    "from collections import OrderedDict\n",
    "\n",
    "\"\"\"\n",
    "Use: python train_ner_alpha.py training_corpus_path model_name\n",
    "Example: python train_ner_alpha data/sample_corpus.txt sample_model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def reformat_train_data(tokenizer, examples):\n",
    "    \"\"\"Reformat data to match JSON format\"\"\"\n",
    "    output = []\n",
    "    for i, (text, entity_offsets) in enumerate(examples):\n",
    "        doc = tokenizer(text)\n",
    "        ner_tags = biluo_tags_from_offsets(tokenizer(text), entity_offsets)\n",
    "        words = [w.text for w in doc]\n",
    "        tags = ['-'] * len(doc)\n",
    "        heads = [0] * len(doc)\n",
    "        deps = [''] * len(doc)\n",
    "        sentence = (range(len(doc)), words, tags, heads, deps, ner_tags)\n",
    "        output.append((text, [(sentence, [])]))\n",
    "    return output\n",
    "\n",
    "\n",
    "def train_ner(train_data, model_dir=None):\n",
    "    \n",
    "    nlp = English(pipeline=['tensorizer', 'ner'])\n",
    "    get_data = lambda: reformat_train_data(nlp.tokenizer, train_data)\n",
    "    optimizer = nlp.begin_training(get_data)\n",
    "    for itn in range(50):\n",
    "        print(\"iteration: \"+str(itn))\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        for raw_text, entity_offsets in train_data:\n",
    "            doc = nlp.make_doc(raw_text)\n",
    "            gold = GoldParse(doc, entities=entity_offsets)\n",
    "            nlp.update(\n",
    "                [doc], # Batch of Doc objects\n",
    "                [gold], # Batch of GoldParse objects\n",
    "                drop=0.5, # Dropout -- make it harder to memorise data\n",
    "                sgd=optimizer, # Callable to update weights\n",
    "                losses=losses)\n",
    "        print(losses)\n",
    "    print(\"Save to\", model_dir)\n",
    "    nlp.to_disk(model_dir)\n",
    "\n",
    "def main(train, model_name):\n",
    "\n",
    "    print(\"training name entity recognizer model...this may take a while\")\n",
    "    start = timeit.default_timer()\n",
    "    train_ner(train, str(model_name)+\"/\")\n",
    "    end = timeit.default_timer()-start\n",
    "    print(\"Training complete with time {} minutes\".format(end/60))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/db_train.txt') as f:\n",
    "    train = list([b.strip('\\n') for b in f])\n",
    "        \n",
    "train = [ast.literal_eval(b) for b in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（悟天）お父さーん。\n"
     ]
    }
   ],
   "source": [
    "print(train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from ja import japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = japanese.Japanese(pipeline=['tensorizer', 'ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training name entity recognizer model...this may take a while\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected unicode, got str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0c5def6755ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-f518df94efbf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train, model_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training name entity recognizer model...this may take a while\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mtrain_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete with time {} minutes\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-f518df94efbf>\u001b[0m in \u001b[0;36mtrain_ner\u001b[0;34m(train_data, model_dir)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tensorizer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ner'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mget_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreformat_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iteration: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/japanese_text/analysis/local/lib/python2.7/site-packages/spacy/language.pyc\u001b[0m in \u001b[0;36mbegin_training\u001b[0;34m(self, get_gold_tuples, **cfg)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNeuralLabeller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;31m# Populate vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannots_brackets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_gold_tuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mannots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannots_brackets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-f518df94efbf>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tensorizer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ner'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mget_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreformat_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-f518df94efbf>\u001b[0m in \u001b[0;36mreformat_train_data\u001b[0;34m(tokenizer, examples)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_offsets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mner_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbiluo_tags_from_offsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_offsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected unicode, got str)"
     ]
    }
   ],
   "source": [
    "main(train, 'db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
